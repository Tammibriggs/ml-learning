{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qa30S6Ec8Odr"
      },
      "outputs": [],
      "source": [
        "#| default_exp training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a4kKAjeD8Odt"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from torch import tensor,nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5AjvqIub8Odt"
      },
      "outputs": [],
      "source": [
        "from fastcore.test import test_close\n",
        "\n",
        "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
        "torch.manual_seed(1)\n",
        "mpl.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
        "path_data = Path('data')\n",
        "path_data.mkdir(exist_ok=True)\n",
        "path_gz = path_data/'mnist.pkl.gz'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n",
        "\n",
        "\n",
        "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n"
      ],
      "metadata": {
        "id": "H41fCyyK9t01"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMKVCBeB90HO",
        "outputId": "b5ba2626-a2fa-43f4-c11a-24b0d56da2ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKwBiq_U8Odu"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAulPk5y8Odv"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3iH7lS1D8Odv"
      },
      "outputs": [],
      "source": [
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "nh = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fvAX52c58Odv"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pc3K9e78Odv",
        "outputId": "bae5d077-d306-42d1-9a51-8ad1d45a5117"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model = Model(m, nh, 10)\n",
        "pred = model(x_train)\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.exp().sum(-1,keepdim=True).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mckoWc-DYDfy",
        "outputId": "284396b0-613b-4e92-cef4-1a83f1169d22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r97tRozV8Odx"
      },
      "source": [
        "### Cross entropy loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsT1gDQa8Odx"
      },
      "source": [
        "First, we will need to compute the softmax of our activations. This is defined by:\n",
        "\n",
        "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
        "\n",
        "or more concisely:\n",
        "\n",
        "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$\n",
        "\n",
        "In practice, we will need the log of the softmax when we calculate the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kUwcI4pe8Odx"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDJhLnJ08Ody",
        "outputId": "82e64755-700d-4207-c7e6-72cc69db1ed8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "log_soft = log_softmax(pred)\n",
        "log_soft.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJtrb9A8Ody"
      },
      "source": [
        "Note that the formula\n",
        "\n",
        "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
        "\n",
        "gives a simplification when we compute the log softmax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QL8Hhf2R8Ody"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH2L8WD68Ody"
      },
      "source": [
        "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
        "\n",
        "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
        "\n",
        "where a is the maximum of the $x_{j}$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_soft.max(-1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evyWG95H4Cwr",
        "outputId": "05c57828-44bb-464f-cc84-ad504443a258"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.19, -2.16, -2.09,  ..., -2.16, -2.17, -2.16], grad_fn=<MaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_soft[49999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vB1SwBk4jC4",
        "outputId": "155ac53e-01c3-41c2-c82e-19ed535eb763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.33, -2.55, -2.36, -2.23, -2.32, -2.22, -2.35, -2.29, -2.27, -2.16], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oAoc9Ib98Odz"
      },
      "outputs": [],
      "source": [
        "def logsumexp(x):\n",
        "    m = x.max(-1)[0]\n",
        "    return m + (x-m[:,None]).exp().sum(-1).log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OplylbG8Odz"
      },
      "source": [
        "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KvkITGVX8Odz"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odt2ji-r8Odz",
        "outputId": "acd1f0ef-5da3-4ccc-ab34-70e6173c2d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "test_close(logsumexp(pred), pred.logsumexp(-1))\n",
        "sm_pred = log_softmax(pred)\n",
        "sm_pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgO_fGDV8Odz"
      },
      "source": [
        "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
        "\n",
        "$$ -\\sum x\\, \\log p(x) $$\n",
        "\n",
        "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
        "\n",
        "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me1Kf4Vy8Odz",
        "outputId": "c769dab9-05d7-4532-ec6d-60e0e44b759d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y_train[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKUe82r8Od0",
        "outputId": "d6d3025a-bebf-4f8f-f132-1f61782ca485"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-2.20, grad_fn=<SelectBackward0>),\n",
              " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
              " tensor(-2.36, grad_fn=<SelectBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "sm_pred[0,5],sm_pred[1,0],sm_pred[2,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vc5Jets8Od0",
        "outputId": "4681306a-e41e-4973-a8a9-8c0d79563a08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "sm_pred[[0,1,2], y_train[:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YR6-RQoK8Od0"
      },
      "outputs": [],
      "source": [
        "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZwceDlz8Od0",
        "outputId": "5a01a8b6-f33f-4c94-db70-0102d9fd0ea9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.30, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "loss = nll(sm_pred, y_train)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHdy6HWz8Od0"
      },
      "source": [
        "Then use PyTorch's implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jg9216FG-C7d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aUbiF08C8Od1"
      },
      "outputs": [],
      "source": [
        "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id5mmTvJ8Od1"
      },
      "source": [
        "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hoJaj1Xk8Od1"
      },
      "outputs": [],
      "source": [
        "test_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21VvJRfk8Od1"
      },
      "source": [
        "## Basic training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gG7fp_V8Od1"
      },
      "source": [
        "Basically the training loop repeats over the following steps:\n",
        "- get the output of the model on a batch of inputs\n",
        "- compare the output to the labels we have and compute a loss\n",
        "- calculate the gradients of the loss with respect to every parameter of the model\n",
        "- update said parameters with those gradients to make them a little bit better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dAQ1IAwD8Od1"
      },
      "outputs": [],
      "source": [
        "loss_func = F.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO4esztB8Od1",
        "outputId": "ceb7b18a-b252-462d-b9ac-6a4b42432e12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
              " torch.Size([50, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "bs=50                  # batch size\n",
        "\n",
        "xb = x_train[0:bs]     # a mini-batch from x\n",
        "preds = model(xb)      # predictions\n",
        "preds[0], preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4XfXPNx8Od2",
        "outputId": "34cd8e13-220b-4f2c-e9bc-8962923a4847"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n",
              "        3, 9, 8, 5, 9, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "yb = y_train[0:bs]\n",
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJNaCyQs8Od2",
        "outputId": "326ea452-4429-4e59-f1dc-ea9a61d372c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.30, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "loss_func(preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.max(dim=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNUC501J_G8K",
        "outputId": "ea7dacb4-26b1-4eb2-e6aa-0b2e46a81f0e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.10, 0.14, 0.21, 0.09, 0.15, 0.17, 0.10, 0.17, 0.06, 0.09, 0.13, 0.06, 0.17, 0.15, 0.08, 0.12, 0.05, 0.11, 0.07, 0.08, 0.16, 0.17,\n",
              "        0.09, 0.10, 0.17, 0.15, 0.07, 0.15, 0.08, 0.07, 0.10, 0.07, 0.14, 0.07, 0.20, 0.14, 0.18, 0.21, 0.09, 0.23, 0.11, 0.09, 0.08, 0.08,\n",
              "        0.09, 0.17, 0.11, 0.12, 0.15, 0.15], grad_fn=<MaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4aNunqD78Od2",
        "outputId": "77af3d80-9c0a-4a10-dee0-01337b75a413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n",
              "        3, 5, 9, 5, 9, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "preds.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "odYfXGa58Od2"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xWB9Evr8Od2",
        "outputId": "27e76bbb-ed8c-4798-f0be-beec07e05135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.08)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "accuracy(preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "26EeVNlB8Od2"
      },
      "outputs": [],
      "source": [
        "lr = 0.5   # learning rate\n",
        "epochs = 3 # how many epochs to train for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0V5H-uKl8Od6"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdQ1P3Lj8Od6",
        "outputId": "bc054e6c-5e06-4118-8d9a-ddb20e28ff3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.30, 0.08\n"
          ]
        }
      ],
      "source": [
        "xb,yb = x_train[:bs],y_train[:bs]\n",
        "preds = model(xb)\n",
        "report(loss_func(preds, yb), preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrofp0Qk8Od7",
        "outputId": "0f5facc5-28b3-4107-e425-bcbb3d90fc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.98\n",
            "0.12, 0.94\n",
            "0.08, 0.96\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for l in model.layers:\n",
        "                if hasattr(l, 'weight'):\n",
        "                    l.weight -= l.weight.grad * lr\n",
        "                    l.bias   -= l.bias.grad   * lr\n",
        "                    l.weight.grad.zero_()\n",
        "                    l.bias  .grad.zero_()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLeRBFNN8Od7"
      },
      "source": [
        "## Using parameters and optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye8vHgHK8Od7"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bC9AMkYF8Od7",
        "outputId": "6cf47b5e-9bab-4de9-d2b3-3ef99ea3b381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Module(\n",
              "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "m1 = nn.Module()\n",
        "m1.foo = nn.Linear(3,4)\n",
        "m1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xeDbLyHS8Od7",
        "outputId": "a0ba4d09-b3a4-4d50-f01d-e7f5ffdfb819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "list(m1.named_children())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "so75BQN98Od7",
        "outputId": "b9ffb601-7d48-473d-9e7d-04bc007ee341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.named_children at 0x7f7f1cf84040>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "m1.named_children()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tUIFwYCW8Od8",
        "outputId": "655efb25-3188-4ce0-f3b0-5303fa076a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.57,  0.43, -0.30],\n",
              "         [ 0.13, -0.32, -0.24],\n",
              "         [ 0.51,  0.04,  0.22],\n",
              "         [ 0.13, -0.17, -0.24]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.01, -0.51, -0.39,  0.56], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "list(m1.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2CHOEiVY8Od8"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x): return self.l2(self.relu(self.l1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "7hSb8KwT8Od8",
        "outputId": "90db28d6-75e3-4441-a045-2f4cc868b39f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=50, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "model = MLP(m, nh, 10)\n",
        "model.l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YyMO7ScE8Od9",
        "outputId": "72c96c90-fedf-4dbd-9d80-c49cad26a339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WmXXxRwF8Od9",
        "outputId": "66d4fabd-3109-4238-ace7-bf544a97ab7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l1: Linear(in_features=784, out_features=50, bias=True)\n",
            "l2: Linear(in_features=50, out_features=10, bias=True)\n",
            "relu: ReLU()\n"
          ]
        }
      ],
      "source": [
        "for name,l in model.named_children(): print(f\"{name}: {l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SN2ume-i8Od9",
        "outputId": "c11c7810-8939-442d-e41d-669b96133c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 784])\n",
            "torch.Size([50])\n",
            "torch.Size([10, 50])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for p in model.parameters(): print(p.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wDinLV8N8Od9"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, n, bs):\n",
        "            s = slice(i, min(n,i+bs))\n",
        "            xb,yb = x_train[s],y_train[s]\n",
        "            preds = model(xb)\n",
        "            loss = loss_func(preds, yb)\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters(): p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "        report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "aItMCcbr8Od-",
        "outputId": "a7a2f420-ca63-4be8-cdb4-e625b8bced71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19, 0.96\n",
            "0.11, 0.96\n",
            "0.04, 1.00\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Irqm_E8Od-"
      },
      "source": [
        "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "j-yPzULv8Od-"
      },
      "outputs": [],
      "source": [
        "class MyModule:\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self._modules = {}\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "\n",
        "    def __setattr__(self,k,v):\n",
        "        if not k.startswith(\"_\"): self._modules[k] = v\n",
        "        super().__setattr__(k,v)\n",
        "\n",
        "    def __repr__(self): return f'{self._modules}'\n",
        "\n",
        "    def parameters(self):\n",
        "        for l in self._modules.values(): yield from l.parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IS_CiuE48Od-",
        "outputId": "6a5645cf-fa99-4871-cbc9-2408acb53189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "mdl = MyModule(m,nh,10)\n",
        "mdl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "YmNI0IFk8Od_",
        "outputId": "391bbebc-e2cf-466a-fc79-fb2b957d1498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 784])\n",
            "torch.Size([50])\n",
            "torch.Size([10, 50])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for p in mdl.parameters(): print(p.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgA_rNOV8Od_"
      },
      "source": [
        "### Registering modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "LArlOHzT8Od_"
      },
      "outputs": [],
      "source": [
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsIUCrlM8Od_"
      },
      "source": [
        "We can use the original `layers` approach, but we have to register the modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "D3LLYGDi8Od_"
      },
      "outputs": [],
      "source": [
        "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "DZVGj5pn8Od_"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
        "\n",
        "    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "f6_4zdE88Od_",
        "outputId": "235709af-6043-41ec-e572-2bed208fa257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (layer_1): ReLU()\n",
              "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model = Model(layers)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NMljWNJ08OeA",
        "outputId": "43ea7633-a5a2-470d-dda5-ff6eb8144bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model(xb).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btluz1lc8OeA"
      },
      "source": [
        "### nn.ModuleList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZfRc3UM8OeA"
      },
      "source": [
        "`nn.ModuleList` does this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xIrjg5Vc8OeA"
      },
      "outputs": [],
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Qp43PB8L8OeA",
        "outputId": "9a337e6c-1049-46f8-c593-5b4c9033dd4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialModel(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "model = SequentialModel(layers)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8WDW9auj8OeA",
        "outputId": "0a78d457-7397-4b62-ccd8-12cd6277d198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.96\n",
            "0.11, 0.96\n",
            "0.07, 0.98\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNBLeAX48OeB"
      },
      "source": [
        "### nn.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59FOf6a-8OeB"
      },
      "source": [
        "`nn.Sequential` is a convenient class which does the same as the above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2oahTCP-8OeB"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ziqPNT3H8OeB",
        "outputId": "0c1d9b7e-b8b4-40f1-a87b-6889455da432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15, 0.96\n",
            "0.11, 0.96\n",
            "0.09, 0.94\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "9Ai3dc528OeB",
        "outputId": "8dad3a19-7954-402d-d50b-bd89f80059e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0arnZTQH8OeB"
      },
      "source": [
        "### optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tP2aBT6n8OeB"
      },
      "outputs": [],
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "\n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for p in self.params: p -= p.grad * self.lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.params: p.grad.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "PMdOahmA8OeC"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ZkPC5E6N8OeC"
      },
      "outputs": [],
      "source": [
        "opt = Optimizer(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "srI0vFlw8OeC",
        "outputId": "c1cd1783-3d0a-45ad-b7d8-e6005ce09f40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18, 0.94\n",
            "0.13, 0.96\n",
            "0.11, 0.94\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48vZGM_8OeC"
      },
      "source": [
        "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zo17vLLp8OeC"
      },
      "outputs": [],
      "source": [
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "gdJgdDgW8OeD"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "IByz9pmU8OeD",
        "outputId": "cb140e22-90ba-4ac1-f353-bee4dbc9a84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.33, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "loss_func(model(xb), yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "EbSZP8L-8OeD",
        "outputId": "093c3583-c20e-49fa-d88a-f2eee38846b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.98\n",
            "0.09, 0.98\n",
            "0.07, 0.98\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        s = slice(i, min(n,i+bs))\n",
        "        xb,yb = x_train[s],y_train[s]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsiR5M-K8OeD"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5OTBjYG8OeD"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QNB-TIQ8OeE"
      },
      "source": [
        "It's clunky to iterate through minibatches of x and y values separately:\n",
        "\n",
        "```python\n",
        "    xb = x_train[s]\n",
        "    yb = y_train[s]\n",
        "```\n",
        "\n",
        "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
        "\n",
        "```python\n",
        "    xb,yb = train_ds[s]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "hOJGihyj8OeE"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i],self.y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Hf_zVz4I8OeE"
      },
      "outputs": [],
      "source": [
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "pp5HGpdN8OeF",
        "outputId": "199bf33e-e304-40e4-f16a-d38289fe1ff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([5, 0, 4, 1, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "xb,yb = train_ds[0:5]\n",
        "assert xb.shape==(5,28*28)\n",
        "assert yb.shape==(5,)\n",
        "xb,yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Y9zcfNng8OeF"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "DkOzQq4q8OeF",
        "outputId": "76e12822-19c4-4a7b-a10d-a31b9ae8b328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.17, 0.96\n",
            "0.11, 0.94\n",
            "0.09, 0.96\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(0, n, bs):\n",
        "        xb,yb = train_ds[i:min(n,i+bs)]\n",
        "        preds = model(xb)\n",
        "        loss = loss_func(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWxFTCP8OeF"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_WiECAc8OeF"
      },
      "source": [
        "Previously, our loop iterated over batches (xb, yb) like this:\n",
        "\n",
        "```python\n",
        "for i in range(0, n, bs):\n",
        "    xb,yb = train_ds[i:min(n,i+bs)]\n",
        "    ...\n",
        "```\n",
        "\n",
        "Let's make our loop much cleaner, using a data loader:\n",
        "\n",
        "```python\n",
        "for xb,yb in train_dl:\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  for i in range(0, len(train_ds), bs):\n",
        "    yield i\n",
        "\n",
        "f =test()"
      ],
      "metadata": {
        "id": "RfYOScLnt1-z"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test1():\n",
        "  return iter(range(0, len(train_ds), bs))\n",
        "\n",
        "f1 =test1()"
      ],
      "metadata": {
        "id": "2bm6hWjfrCLW"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfuZnVj8tX02"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "EA9xsxoa8OeG"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "r5iHQS2O8OeG"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs)\n",
        "valid_dl = DataLoader(valid_ds, bs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = list(valid_dl)"
      ],
      "metadata": {
        "id": "WGWwkx7jvkoT"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "sPkrw0Md8OeG",
        "outputId": "15c9026c-0ea7-45e2-cee6-f26e69b975dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ],
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "xb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "AI5-zmY78OeG",
        "outputId": "4e1fe486-5097-40e8-f264-e2e12465506d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n",
              "        8, 3, 7, 7, 8, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ],
      "source": [
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "gu2ZsLyS8OeH",
        "outputId": "e788001a-33d1-43c7-d31f-ceac4521b37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 242
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "y6yATDgb8OeH"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "Qr5k2d238OeH"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            preds = model(xb)\n",
        "            loss = loss_func(preds, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        report(loss, preds, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "lwa4MVDG8OeI",
        "outputId": "c9d56dd6-f34b-45af-d9c1-073d8f2f9346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11, 0.98\n",
            "0.09, 0.98\n",
            "0.06, 1.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ],
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO9pM1qo8OeI"
      },
      "source": [
        "### Random sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6TSVwTJ8OeI"
      },
      "source": [
        "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "AY-RA9rM8OeI"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "zNW3Q78q8OeI"
      },
      "outputs": [],
      "source": [
        "class Sampler():\n",
        "    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n",
        "    def __iter__(self):\n",
        "        res = list(range(self.n))\n",
        "        if self.shuffle: random.shuffle(res)\n",
        "        return iter(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "C5LVamh18OeJ"
      },
      "outputs": [],
      "source": [
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "0Tubq10d8OeJ"
      },
      "outputs": [],
      "source": [
        "ss = Sampler(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "G9IHUA9O8OeJ",
        "outputId": "92b70d27-e828-4e7f-9d7f-780f61a6a870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "it = iter(ss)\n",
        "for o in range(5): print(next(it))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Hwg6LiUA8OeJ",
        "outputId": "642014a8-0705-4c00-9fe3-836bfe19997d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "list(islice(ss, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "fusTWZxr8OeJ",
        "outputId": "ec1caa72-40ad-485e-bc1b-6fd3815402e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25295, 34861, 14669, 40124, 26341]"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "ss = Sampler(train_ds, shuffle=True)\n",
        "list(islice(ss, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "d9ORHirX8OeJ"
      },
      "outputs": [],
      "source": [
        "import fastcore.all as fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "Jy9NLqwn8OeK"
      },
      "outputs": [],
      "source": [
        "class BatchSampler():\n",
        "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
        "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "SThmvLik8OeK",
        "outputId": "7e9d51f1-cec0-4c1c-8c81-e8915f5e5787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[45956, 14963, 31051, 37867],\n",
              " [12720, 40651, 31431, 9363],\n",
              " [7256, 25286, 3552, 9051],\n",
              " [13435, 27068, 18287, 37548],\n",
              " [49230, 13438, 9366, 43319]]"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "batchs = BatchSampler(ss, 4)\n",
        "list(islice(batchs, 5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(batchs))"
      ],
      "metadata": {
        "id": "18pmOzg42sgd",
        "outputId": "e921bccd-da68-4e03-97d5-c0e0c086cad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37632, 47131, 23370, 17600]"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(batchs)"
      ],
      "metadata": {
        "id": "EyCPB3bRSjBr",
        "outputId": "6d5e4de9-d0b0-43f6-b726-0966a9a14f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[35391, 22217, 21279, 43389],\n",
              " [25431, 28038, 45921, 8098],\n",
              " [8768, 13440, 40554, 31248],\n",
              " [17233, 23613, 10960, 39162],\n",
              " [3786, 17485, 10428, 41380],\n",
              " [25657, 2382, 13338, 12426],\n",
              " [6477, 23291, 27468, 22218],\n",
              " [6759, 30799, 44857, 36834],\n",
              " [13640, 6537, 38540, 46011],\n",
              " [21516, 20604, 22479, 29720],\n",
              " [45146, 27055, 32829, 36682],\n",
              " [41576, 31561, 10010, 10389],\n",
              " [8802, 18537, 32543, 45221],\n",
              " [27657, 31414, 27421, 18862],\n",
              " [47859, 45218, 36963, 27959],\n",
              " [20031, 45230, 7447, 4176],\n",
              " [38986, 46941, 10333, 256],\n",
              " [7619, 40122, 16514, 21506],\n",
              " [28759, 6142, 28601, 35110],\n",
              " [3654, 40588, 37774, 7020],\n",
              " [36457, 1241, 22610, 48824],\n",
              " [34368, 24026, 7159, 46426],\n",
              " [3933, 38566, 31166, 38072],\n",
              " [43149, 28457, 11336, 2089],\n",
              " [4046, 17539, 27402, 43168],\n",
              " [47430, 33131, 40816, 6046],\n",
              " [22922, 45067, 27905, 36595],\n",
              " [33095, 296, 20739, 18502],\n",
              " [40794, 25602, 28128, 25953],\n",
              " [379, 17175, 35173, 7478],\n",
              " [9868, 40106, 24005, 29642],\n",
              " [2060, 45213, 26205, 30626],\n",
              " [47388, 45274, 232, 48565],\n",
              " [41054, 44307, 46049, 36812],\n",
              " [27356, 22522, 41896, 18515],\n",
              " [24680, 38013, 37900, 44013],\n",
              " [18677, 47526, 24125, 23689],\n",
              " [38910, 37772, 41023, 42102],\n",
              " [15876, 35857, 13916, 47593],\n",
              " [4244, 1426, 12252, 46517],\n",
              " [45038, 38170, 41246, 6755],\n",
              " [13533, 28189, 37187, 14490],\n",
              " [31901, 12012, 15487, 9460],\n",
              " [15737, 22361, 38526, 4568],\n",
              " [45248, 1647, 1292, 13992],\n",
              " [2516, 23628, 24758, 44286],\n",
              " [43099, 23562, 13805, 19445],\n",
              " [4367, 22394, 12583, 27141],\n",
              " [22927, 44432, 8545, 6601],\n",
              " [24527, 11467, 5540, 4347],\n",
              " [31753, 19658, 41300, 16788],\n",
              " [21125, 47971, 9971, 17957],\n",
              " [10487, 15403, 30035, 47827],\n",
              " [37305, 31168, 35641, 898],\n",
              " [40102, 28390, 22261, 34160],\n",
              " [3294, 32508, 29695, 41312],\n",
              " [19593, 16458, 1575, 48842],\n",
              " [38438, 5094, 9534, 11955],\n",
              " [20318, 46469, 19263, 43142],\n",
              " [7873, 48845, 6409, 9869],\n",
              " [49396, 8474, 47138, 21427],\n",
              " [39848, 6921, 42245, 45322],\n",
              " [49329, 33528, 29421, 36247],\n",
              " [5351, 43892, 15933, 23840],\n",
              " [9370, 49759, 1177, 1893],\n",
              " [12489, 16988, 37605, 1173],\n",
              " [32105, 27700, 8349, 16546],\n",
              " [2761, 21824, 42392, 29707],\n",
              " [26773, 13303, 28632, 27338],\n",
              " [42402, 40957, 40512, 16998],\n",
              " [27023, 25395, 28182, 42621],\n",
              " [44331, 43499, 43527, 1810],\n",
              " [40496, 17442, 43558, 22054],\n",
              " [8708, 34880, 46815, 1912],\n",
              " [40321, 1735, 29613, 25994],\n",
              " [5271, 8766, 1828, 45044],\n",
              " [5949, 7669, 30301, 33947],\n",
              " [8934, 10046, 2818, 9217],\n",
              " [4751, 22486, 37832, 33640],\n",
              " [20963, 1121, 12627, 343],\n",
              " [972, 31613, 5277, 5882],\n",
              " [20162, 40612, 41922, 23870],\n",
              " [13385, 1580, 31155, 18676],\n",
              " [48110, 1751, 47990, 12904],\n",
              " [35526, 44156, 33977, 7755],\n",
              " [9640, 1268, 36839, 18458],\n",
              " [35433, 30353, 3075, 4683],\n",
              " [43876, 14287, 25120, 17897],\n",
              " [21182, 5333, 38677, 33725],\n",
              " [47795, 40436, 26015, 11311],\n",
              " [2073, 23768, 43703, 14355],\n",
              " [24651, 49726, 44232, 9308],\n",
              " [10039, 17753, 36284, 2508],\n",
              " [26534, 3514, 39126, 559],\n",
              " [28239, 22262, 48729, 33391],\n",
              " [2703, 32823, 24043, 38421],\n",
              " [27558, 14155, 5658, 26504],\n",
              " [27000, 3048, 2867, 7799],\n",
              " [14922, 5460, 35672, 1794],\n",
              " [10867, 38817, 34440, 23406],\n",
              " [28431, 10594, 31867, 41812],\n",
              " [4253, 32742, 35281, 2331],\n",
              " [3346, 18709, 31620, 7025],\n",
              " [9367, 38497, 29595, 49590],\n",
              " [1793, 32064, 46416, 30927],\n",
              " [22087, 42862, 49054, 2669],\n",
              " [48552, 32012, 14452, 18823],\n",
              " [33900, 47387, 31151, 37731],\n",
              " [32468, 3550, 31284, 2685],\n",
              " [41835, 26907, 23818, 48935],\n",
              " [9043, 11325, 36141, 30062],\n",
              " [34147, 23319, 30890, 42133],\n",
              " [44625, 44153, 25324, 15850],\n",
              " [39262, 49265, 13151, 15380],\n",
              " [30318, 33102, 26101, 4505],\n",
              " [25918, 30300, 41226, 46236],\n",
              " [44973, 26644, 45194, 35041],\n",
              " [33425, 30293, 30534, 40640],\n",
              " [18973, 20513, 6392, 19686],\n",
              " [30059, 34050, 48905, 39780],\n",
              " [17341, 38243, 35485, 22358],\n",
              " [34879, 32094, 47540, 6368],\n",
              " [45752, 9031, 22675, 27565],\n",
              " [7661, 47757, 22563, 27439],\n",
              " [10822, 26984, 49361, 10311],\n",
              " [16425, 14004, 12275, 26642],\n",
              " [38858, 24152, 45684, 10583],\n",
              " [22232, 46616, 25345, 16370],\n",
              " [23330, 24278, 4626, 12094],\n",
              " [42143, 17349, 887, 29437],\n",
              " [48701, 6187, 7320, 36334],\n",
              " [42367, 49967, 4635, 1737],\n",
              " [4166, 48787, 37802, 29676],\n",
              " [19622, 3248, 30339, 25304],\n",
              " [3245, 37716, 5488, 26262],\n",
              " [14107, 19911, 6147, 16405],\n",
              " [7365, 88, 49935, 31059],\n",
              " [18658, 47284, 20374, 33317],\n",
              " [25277, 33932, 26082, 40792],\n",
              " [47200, 4294, 36406, 49887],\n",
              " [11884, 14279, 19408, 46460],\n",
              " [19781, 28794, 6880, 40333],\n",
              " [3142, 12933, 10104, 7487],\n",
              " [47831, 36442, 6577, 2473],\n",
              " [35002, 18465, 12832, 49811],\n",
              " [40111, 7483, 37076, 37994],\n",
              " [19107, 22085, 995, 32680],\n",
              " [38612, 3253, 9229, 194],\n",
              " [25006, 10404, 36733, 44287],\n",
              " [27835, 23141, 7708, 15882],\n",
              " [5323, 39797, 573, 43643],\n",
              " [25511, 47864, 7000, 10778],\n",
              " [33467, 32506, 45100, 27075],\n",
              " [33809, 30109, 2187, 37102],\n",
              " [11822, 7615, 16742, 6406],\n",
              " [38804, 1439, 8431, 588],\n",
              " [30065, 41309, 42909, 35817],\n",
              " [5490, 45331, 46977, 26303],\n",
              " [47384, 655, 37447, 34180],\n",
              " [46762, 2072, 5430, 33386],\n",
              " [29384, 49180, 49994, 13112],\n",
              " [39718, 21843, 2140, 22561],\n",
              " [21572, 40779, 13777, 31144],\n",
              " [2629, 35892, 31221, 15252],\n",
              " [34272, 32333, 8310, 22982],\n",
              " [9142, 9181, 34176, 28254],\n",
              " [36082, 27754, 38418, 16140],\n",
              " [23869, 452, 14570, 44362],\n",
              " [19497, 44649, 30936, 5045],\n",
              " [46317, 8609, 17460, 2114],\n",
              " [49975, 28868, 2739, 22312],\n",
              " [527, 7995, 16227, 6972],\n",
              " [45842, 12600, 39213, 9686],\n",
              " [33040, 14226, 23295, 31708],\n",
              " [49628, 24874, 22521, 27755],\n",
              " [43791, 43062, 25920, 3412],\n",
              " [14467, 6768, 6415, 23482],\n",
              " [12030, 16173, 49669, 40558],\n",
              " [6115, 26371, 40482, 30038],\n",
              " [48732, 7938, 48968, 39566],\n",
              " [37904, 46198, 13446, 18340],\n",
              " [1661, 32195, 18272, 10078],\n",
              " [14892, 28242, 47948, 33998],\n",
              " [39716, 13933, 11937, 30047],\n",
              " [30672, 4104, 24014, 4271],\n",
              " [28950, 44795, 26985, 22103],\n",
              " [38498, 19905, 14774, 46379],\n",
              " [33309, 26682, 24524, 25542],\n",
              " [31725, 21972, 19362, 6075],\n",
              " [11226, 3067, 39658, 6828],\n",
              " [36373, 22399, 48844, 7662],\n",
              " [9945, 38103, 6246, 6315],\n",
              " [28314, 11904, 21598, 9392],\n",
              " [37064, 34173, 6729, 42223],\n",
              " [6527, 3604, 41651, 11356],\n",
              " [44751, 12703, 20793, 44510],\n",
              " [19862, 21976, 2251, 13047],\n",
              " [14820, 1685, 41552, 28764],\n",
              " [20678, 34681, 44164, 21649],\n",
              " [26397, 48334, 37601, 13784],\n",
              " [37213, 10068, 32563, 42051],\n",
              " [24183, 37565, 4484, 42425],\n",
              " [49845, 38307, 25400, 41824],\n",
              " [11778, 15454, 764, 35915],\n",
              " [3256, 25630, 30412, 2862],\n",
              " [32148, 46163, 15662, 18151],\n",
              " [9189, 42728, 14588, 39659],\n",
              " [22515, 39462, 13575, 31048],\n",
              " [36608, 39541, 15544, 17689],\n",
              " [20761, 34976, 25332, 17820],\n",
              " [47382, 15339, 46387, 25686],\n",
              " [31826, 9675, 44680, 39646],\n",
              " [27669, 404, 14939, 26000],\n",
              " [14008, 36109, 11225, 38377],\n",
              " [33848, 11393, 25437, 35815],\n",
              " [41154, 40192, 2734, 29660],\n",
              " [7591, 38236, 3903, 36540],\n",
              " [457, 32037, 20927, 8350],\n",
              " [35018, 27250, 11387, 9391],\n",
              " [10545, 48897, 12566, 46598],\n",
              " [20431, 17373, 13751, 24505],\n",
              " [21094, 5975, 3505, 18742],\n",
              " [25841, 32903, 49554, 29561],\n",
              " [4188, 9665, 430, 38974],\n",
              " [8641, 35408, 8223, 40347],\n",
              " [15008, 7080, 33839, 3655],\n",
              " [14598, 14471, 13472, 159],\n",
              " [23599, 33702, 44659, 46350],\n",
              " [47177, 37783, 17071, 8132],\n",
              " [20052, 17749, 34745, 32782],\n",
              " [37974, 40572, 36807, 34163],\n",
              " [1698, 4486, 3821, 20620],\n",
              " [27327, 4164, 16355, 40044],\n",
              " [8134, 18427, 809, 30815],\n",
              " [47887, 5731, 18941, 25241],\n",
              " [22247, 6120, 32460, 40163],\n",
              " [40562, 14893, 49388, 18241],\n",
              " [20127, 30239, 42882, 43899],\n",
              " [49795, 9052, 39091, 48060],\n",
              " [49055, 28572, 6241, 4631],\n",
              " [4977, 36903, 44811, 46184],\n",
              " [46224, 21317, 31709, 42312],\n",
              " [42022, 14150, 26387, 32566],\n",
              " [9647, 22201, 17343, 34808],\n",
              " [43611, 20838, 30075, 2494],\n",
              " [28370, 27280, 8424, 18521],\n",
              " [34521, 33002, 9026, 45026],\n",
              " [23074, 27673, 39376, 8741],\n",
              " [20023, 6875, 22813, 30266],\n",
              " [30920, 1508, 33476, 25652],\n",
              " [29922, 45603, 4598, 49024],\n",
              " [31963, 10257, 1561, 910],\n",
              " [31161, 48875, 49713, 37888],\n",
              " [13616, 29825, 24382, 10730],\n",
              " [10420, 19534, 40935, 37353],\n",
              " [17260, 24880, 26560, 14447],\n",
              " [48352, 14339, 18231, 33971],\n",
              " [47027, 21406, 49165, 12241],\n",
              " [703, 37033, 21004, 43469],\n",
              " [39330, 35346, 35543, 5983],\n",
              " [22305, 28409, 28310, 18029],\n",
              " [20733, 15613, 3533, 48168],\n",
              " [27666, 1185, 48738, 28959],\n",
              " [21685, 47135, 43736, 42620],\n",
              " [11700, 40592, 37227, 33963],\n",
              " [11671, 5746, 9613, 6444],\n",
              " [38723, 24624, 31820, 9495],\n",
              " [16710, 38605, 16875, 32608],\n",
              " [29358, 21285, 10606, 38385],\n",
              " [15410, 24155, 28878, 43009],\n",
              " [49173, 40095, 15504, 3273],\n",
              " [10872, 724, 14479, 45457],\n",
              " [1558, 49077, 1094, 8825],\n",
              " [40450, 39712, 1061, 10805],\n",
              " [11112, 15473, 25809, 14862],\n",
              " [9511, 28045, 48043, 20936],\n",
              " [30981, 25779, 42520, 26680],\n",
              " [6053, 26627, 49106, 36408],\n",
              " [34108, 17188, 367, 43449],\n",
              " [21832, 16673, 21999, 5682],\n",
              " [37442, 3405, 15028, 3390],\n",
              " [4301, 13623, 19446, 6253],\n",
              " [9764, 22509, 15130, 30155],\n",
              " [8817, 21933, 44600, 23234],\n",
              " [42977, 24970, 12358, 36512],\n",
              " [18621, 12150, 36145, 28486],\n",
              " [4653, 49916, 32612, 14255],\n",
              " [41888, 32387, 31469, 33085],\n",
              " [33376, 20079, 11845, 28399],\n",
              " [41700, 24535, 5699, 35652],\n",
              " [42907, 19698, 2746, 12888],\n",
              " [16259, 24964, 15104, 26597],\n",
              " [21099, 8014, 49791, 21437],\n",
              " [8763, 6210, 49005, 28832],\n",
              " [40210, 22329, 5365, 8576],\n",
              " [43727, 38446, 42782, 41709],\n",
              " [12129, 48070, 36151, 21060],\n",
              " [8723, 10053, 40200, 30861],\n",
              " [3127, 4360, 3512, 23276],\n",
              " [18357, 44654, 38719, 28550],\n",
              " [4590, 6132, 21678, 19722],\n",
              " [9853, 37911, 49323, 8076],\n",
              " [31251, 21525, 14434, 45404],\n",
              " [32132, 29273, 17246, 35612],\n",
              " [48666, 37282, 34373, 19784],\n",
              " [49423, 31955, 28936, 2956],\n",
              " [22594, 25062, 1058, 48783],\n",
              " [34288, 5902, 37999, 10035],\n",
              " [29553, 45875, 1578, 30169],\n",
              " [21320, 41975, 23725, 33477],\n",
              " [41599, 49184, 38631, 36241],\n",
              " [33834, 47688, 37314, 14509],\n",
              " [6920, 7189, 42468, 42504],\n",
              " [25075, 42532, 48241, 42075],\n",
              " [11013, 16179, 38369, 3881],\n",
              " [2325, 5019, 36132, 27045],\n",
              " [21766, 10431, 45394, 18298],\n",
              " [7077, 35125, 33844, 20717],\n",
              " [12348, 12859, 2576, 22019],\n",
              " [18406, 24905, 22021, 33685],\n",
              " [17109, 43040, 42462, 34846],\n",
              " [47095, 41880, 36473, 32131],\n",
              " [16610, 20057, 26115, 3243],\n",
              " [7082, 44923, 6864, 42960],\n",
              " [49614, 24824, 36324, 47723],\n",
              " [47695, 31813, 35917, 5705],\n",
              " [16838, 42760, 46587, 30835],\n",
              " [5309, 31849, 41375, 1457],\n",
              " [46523, 24801, 39559, 39739],\n",
              " [43141, 19684, 28740, 15957],\n",
              " [1155, 14194, 11322, 30248],\n",
              " [14189, 41323, 12922, 17126],\n",
              " [14759, 25852, 25803, 8423],\n",
              " [44073, 24248, 44011, 37138],\n",
              " [24665, 44321, 21363, 19443],\n",
              " [24937, 18800, 44206, 13060],\n",
              " [9788, 500, 44123, 34369],\n",
              " [18898, 31243, 41673, 16680],\n",
              " [4137, 7716, 26577, 46024],\n",
              " [48271, 2462, 20313, 18966],\n",
              " [23487, 32999, 16935, 40124],\n",
              " [16618, 19135, 39631, 38489],\n",
              " [14640, 33613, 36528, 23786],\n",
              " [48492, 20124, 41220, 46542],\n",
              " [38534, 9411, 10739, 18635],\n",
              " [20769, 1242, 3239, 15812],\n",
              " [23332, 2305, 48866, 45911],\n",
              " [1918, 38772, 15703, 16612],\n",
              " [41181, 48988, 3667, 44179],\n",
              " [165, 33793, 17058, 7924],\n",
              " [1004, 49142, 46895, 31283],\n",
              " [5661, 37265, 5442, 47501],\n",
              " [23250, 25863, 14885, 12510],\n",
              " [21164, 4519, 41845, 4970],\n",
              " [3292, 34056, 20984, 16677],\n",
              " [42690, 35626, 15474, 41724],\n",
              " [6404, 46221, 4810, 2031],\n",
              " [8109, 15098, 7622, 37910],\n",
              " [24644, 32802, 49774, 37433],\n",
              " [18717, 24429, 31041, 9754],\n",
              " [19328, 47172, 44691, 2678],\n",
              " [18276, 49952, 40997, 22086],\n",
              " [27831, 12989, 30598, 37105],\n",
              " [28155, 42583, 17638, 2667],\n",
              " [12139, 27684, 30153, 45753],\n",
              " [26318, 3977, 45997, 28825],\n",
              " [38865, 6636, 32617, 29110],\n",
              " [32968, 31879, 24491, 34389],\n",
              " [6893, 3368, 23794, 26284],\n",
              " [21650, 4125, 43401, 18177],\n",
              " [2336, 42642, 29904, 14664],\n",
              " [204, 45445, 20660, 38942],\n",
              " [27647, 29983, 18902, 37191],\n",
              " [28770, 32032, 33316, 8156],\n",
              " [39982, 18470, 21189, 43187],\n",
              " [16052, 45475, 34035, 20890],\n",
              " [3453, 4097, 46009, 25605],\n",
              " [22534, 35306, 49102, 32880],\n",
              " [11184, 7534, 39750, 31890],\n",
              " [28927, 41482, 17448, 37614],\n",
              " [22322, 46836, 44402, 8021],\n",
              " [6234, 33715, 4783, 46912],\n",
              " [3705, 31687, 8955, 14325],\n",
              " [48401, 37341, 35976, 11693],\n",
              " [33898, 40290, 255, 36119],\n",
              " [10812, 31717, 15687, 34451],\n",
              " [30846, 13095, 27505, 34064],\n",
              " [28259, 31206, 9552, 48587],\n",
              " [8042, 33648, 32289, 45117],\n",
              " [24306, 15766, 13806, 21329],\n",
              " [44319, 35649, 25682, 40790],\n",
              " [45343, 141, 13888, 42130],\n",
              " [44761, 48460, 15010, 42192],\n",
              " [27821, 38738, 13014, 42499],\n",
              " [42816, 34018, 14719, 45071],\n",
              " [8837, 30819, 9314, 49646],\n",
              " [11271, 41715, 41048, 2863],\n",
              " [35895, 19295, 46137, 4380],\n",
              " [29959, 22155, 42754, 23914],\n",
              " [30670, 11338, 10632, 23960],\n",
              " [34081, 22905, 22586, 17205],\n",
              " [31434, 34224, 36871, 12246],\n",
              " [31224, 47050, 15730, 21693],\n",
              " [26058, 47738, 40655, 250],\n",
              " [42809, 4323, 18310, 10522],\n",
              " [4255, 45511, 18478, 30376],\n",
              " [47830, 34385, 33183, 18072],\n",
              " [27147, 36525, 13560, 38092],\n",
              " [23561, 3201, 35714, 23404],\n",
              " [6209, 34622, 29924, 38983],\n",
              " [49075, 19378, 7856, 17767],\n",
              " [29004, 46328, 6048, 42253],\n",
              " [14840, 8697, 31389, 15117],\n",
              " [8402, 20918, 45967, 36013],\n",
              " [35537, 35339, 26926, 6154],\n",
              " [28134, 22960, 22318, 32558],\n",
              " [3839, 1573, 22731, 10796],\n",
              " [4382, 9985, 17510, 15565],\n",
              " [35534, 13124, 21797, 16796],\n",
              " [22781, 431, 40284, 36678],\n",
              " [19003, 27941, 11632, 15453],\n",
              " [39455, 33505, 41178, 16741],\n",
              " [39481, 40285, 30723, 42773],\n",
              " [20903, 41411, 39280, 24369],\n",
              " [20522, 49354, 17631, 11470],\n",
              " [18988, 36130, 17167, 21237],\n",
              " [5926, 16261, 23400, 32366],\n",
              " [32329, 14040, 36901, 27442],\n",
              " [44898, 46126, 28051, 6243],\n",
              " [18138, 42662, 3825, 35353],\n",
              " [49963, 29711, 19448, 43832],\n",
              " [33758, 19723, 856, 46852],\n",
              " [15179, 41900, 49729, 25834],\n",
              " [19541, 45968, 35493, 23896],\n",
              " [16551, 10490, 32740, 42900],\n",
              " [48465, 14326, 2531, 44479],\n",
              " [23221, 16403, 32014, 42899],\n",
              " [5335, 40220, 18561, 14474],\n",
              " [26905, 23965, 48175, 22648],\n",
              " [3161, 47245, 3093, 4468],\n",
              " [32824, 34427, 1545, 29670],\n",
              " [21611, 1539, 44883, 13646],\n",
              " [38213, 26448, 34921, 46796],\n",
              " [30496, 5686, 27440, 46434],\n",
              " [19816, 13160, 19112, 1700],\n",
              " [46832, 1513, 2148, 21451],\n",
              " [45203, 41052, 22128, 9982],\n",
              " [11319, 5426, 1802, 10817],\n",
              " [40736, 45525, 44265, 44859],\n",
              " [26188, 21777, 16272, 44023],\n",
              " [15747, 8224, 40922, 31600],\n",
              " [43605, 1769, 36563, 45695],\n",
              " [7023, 7197, 1067, 49261],\n",
              " [39296, 8440, 48033, 25365],\n",
              " [33973, 13935, 36256, 32059],\n",
              " [6328, 713, 14869, 47681],\n",
              " [34004, 28644, 6175, 26661],\n",
              " [33263, 24228, 47648, 40663],\n",
              " [24930, 5196, 30505, 26772],\n",
              " [29596, 33211, 38784, 49212],\n",
              " [35577, 8467, 40196, 11835],\n",
              " [33331, 30324, 5862, 10339],\n",
              " [3149, 35861, 46235, 33178],\n",
              " [38712, 32594, 8246, 14036],\n",
              " [47801, 49501, 25965, 32428],\n",
              " [32244, 11531, 22708, 12554],\n",
              " [21168, 3724, 22076, 47274],\n",
              " [20330, 23858, 37681, 8250],\n",
              " [15552, 41823, 16435, 19659],\n",
              " [43473, 47885, 6720, 14456],\n",
              " [14134, 23167, 22441, 38515],\n",
              " [36691, 28247, 45239, 49197],\n",
              " [6223, 48149, 20652, 14316],\n",
              " [12961, 26676, 19989, 20666],\n",
              " [40799, 25615, 10111, 33454],\n",
              " [10001, 30238, 46763, 4234],\n",
              " [10033, 28767, 39518, 45370],\n",
              " [22390, 15790, 42749, 13471],\n",
              " [1228, 39465, 21880, 49993],\n",
              " [18565, 1641, 37711, 13598],\n",
              " [34277, 41498, 19222, 41528],\n",
              " [32092, 29480, 798, 43950],\n",
              " [24918, 44590, 26120, 49108],\n",
              " [22736, 28287, 27412, 47883],\n",
              " [33364, 3544, 24971, 2227],\n",
              " [2524, 47618, 34321, 3278],\n",
              " [5444, 3168, 5804, 32966],\n",
              " [1907, 49619, 22002, 47446],\n",
              " [26075, 4877, 24626, 41014],\n",
              " [40976, 1966, 41099, 28220],\n",
              " [22047, 48841, 33736, 11347],\n",
              " [49078, 21515, 27636, 23493],\n",
              " [47517, 15320, 38786, 5843],\n",
              " [40850, 42234, 10807, 4252],\n",
              " [24807, 43478, 732, 13619],\n",
              " [26416, 46282, 18698, 21193],\n",
              " [20248, 6377, 9028, 7985],\n",
              " [42458, 4535, 8886, 39692],\n",
              " [43357, 31869, 3179, 31156],\n",
              " [20589, 21364, 24776, 41209],\n",
              " [16138, 30924, 6896, 3018],\n",
              " [38684, 41800, 20379, 29682],\n",
              " [34450, 32426, 16994, 14699],\n",
              " [42268, 7145, 49812, 33250],\n",
              " [4856, 43371, 31633, 18393],\n",
              " [16809, 1551, 43725, 34940],\n",
              " [43999, 44032, 19792, 18725],\n",
              " [44864, 12907, 16899, 42031],\n",
              " [47186, 4013, 41596, 41945],\n",
              " [7215, 2131, 17509, 35365],\n",
              " [18369, 7528, 9684, 23226],\n",
              " [13885, 16445, 18704, 47529],\n",
              " [4174, 12578, 17239, 37606],\n",
              " [8429, 39510, 29979, 38535],\n",
              " [16127, 25794, 37366, 17524],\n",
              " [39075, 30165, 14698, 6271],\n",
              " [16286, 24165, 41006, 46804],\n",
              " [11987, 44622, 12979, 17964],\n",
              " [27314, 67, 5492, 17963],\n",
              " [28458, 20768, 31724, 11800],\n",
              " [24907, 28899, 37695, 36523],\n",
              " [30558, 35156, 31233, 5601],\n",
              " [36862, 14580, 35258, 46844],\n",
              " [19887, 34313, 8804, 38332],\n",
              " [40050, 28660, 8573, 5177],\n",
              " [46229, 36388, 11568, 44244],\n",
              " [9749, 31513, 808, 40239],\n",
              " [28504, 5070, 25171, 109],\n",
              " [23097, 32834, 12545, 12176],\n",
              " [38970, 44078, 13147, 34401],\n",
              " [47432, 25102, 18057, 9882],\n",
              " [13484, 41941, 27720, 3769],\n",
              " [7538, 18064, 27538, 9875],\n",
              " [42413, 41849, 26781, 26048],\n",
              " [45211, 27998, 24217, 48779],\n",
              " [11353, 47637, 23761, 41451],\n",
              " [30493, 31178, 39060, 42290],\n",
              " [18894, 45780, 49490, 25729],\n",
              " [31531, 30481, 38229, 7818],\n",
              " [4159, 39174, 36257, 30126],\n",
              " [39968, 9085, 6109, 24887],\n",
              " [40650, 39078, 45169, 41426],\n",
              " [43425, 17176, 41013, 636],\n",
              " [47279, 18192, 42814, 14673],\n",
              " [27946, 5027, 24534, 21579],\n",
              " [619, 36841, 48562, 41051],\n",
              " [24347, 41632, 34232, 31892],\n",
              " [782, 47508, 38511, 32067],\n",
              " [22870, 14729, 15826, 27847],\n",
              " [19531, 34542, 41646, 45111],\n",
              " [10938, 12981, 47364, 2877],\n",
              " [31428, 4758, 46519, 39911],\n",
              " [44810, 48287, 45469, 39857],\n",
              " [48229, 17346, 33878, 7767],\n",
              " [25193, 44275, 19753, 38168],\n",
              " [44106, 27333, 5401, 43285],\n",
              " [8060, 10814, 21798, 40591],\n",
              " [22841, 29149, 7889, 28661],\n",
              " [17706, 24470, 7545, 26410],\n",
              " [34739, 33006, 16237, 46839],\n",
              " [11105, 39026, 41751, 30963],\n",
              " [26815, 46432, 32099, 3664],\n",
              " [32154, 13106, 47204, 25018],\n",
              " [7858, 29622, 26267, 18600],\n",
              " [17288, 19334, 2847, 40113],\n",
              " [22306, 29300, 3337, 35401],\n",
              " [30703, 16519, 22945, 35142],\n",
              " [14229, 48951, 6365, 40797],\n",
              " [30878, 33765, 34275, 40590],\n",
              " [13556, 18919, 12407, 33444],\n",
              " [14031, 5010, 47374, 4623],\n",
              " [39910, 4433, 19819, 35693],\n",
              " [26947, 28516, 32120, 31337],\n",
              " [14876, 14587, 4811, 15759],\n",
              " [46072, 8443, 43964, 43294],\n",
              " [8146, 47429, 20974, 38949],\n",
              " [38184, 4415, 46872, 16218],\n",
              " [7324, 26951, 16956, 3351],\n",
              " [321, 29405, 10910, 11977],\n",
              " [15243, 22380, 49639, 36704],\n",
              " [11262, 41847, 41690, 19462],\n",
              " [33905, 42308, 13766, 10725],\n",
              " [22589, 26544, 41524, 29306],\n",
              " [3022, 34002, 14410, 33020],\n",
              " [13164, 557, 42989, 47004],\n",
              " [36147, 1681, 12691, 16319],\n",
              " [27931, 23556, 43296, 46744],\n",
              " [24411, 35402, 9008, 21754],\n",
              " [42739, 36986, 19482, 44350],\n",
              " [5076, 40841, 42845, 13063],\n",
              " [47628, 3322, 33597, 8881],\n",
              " [11736, 12955, 35940, 47553],\n",
              " [12709, 19242, 18942, 9196],\n",
              " [43770, 46082, 37829, 28079],\n",
              " [32296, 34125, 25632, 18583],\n",
              " [47822, 8208, 5207, 3184],\n",
              " [40553, 2129, 12564, 7210],\n",
              " [1072, 18161, 41614, 36805],\n",
              " [13557, 32026, 32539, 6919],\n",
              " [21676, 49370, 34057, 24688],\n",
              " [25356, 12936, 47858, 11229],\n",
              " [38186, 6427, 40994, 16712],\n",
              " [30197, 40993, 48643, 5100],\n",
              " [5362, 39343, 6378, 42657],\n",
              " [28015, 33906, 21444, 33065],\n",
              " [5484, 46970, 49540, 26194],\n",
              " [1998, 42044, 1095, 42687],\n",
              " [8795, 13061, 9978, 32781],\n",
              " [36502, 39762, 36380, 33785],\n",
              " [40010, 11286, 33207, 4892],\n",
              " [14994, 48163, 38312, 45177],\n",
              " [11446, 49617, 13548, 3218],\n",
              " [35704, 37425, 3546, 1296],\n",
              " [25463, 43681, 20192, 24479],\n",
              " [46437, 5552, 986, 15899],\n",
              " [16553, 24557, 23101, 26628],\n",
              " [28116, 1348, 32554, 19151],\n",
              " [12903, 46641, 33422, 49739],\n",
              " [45014, 16309, 17110, 48956],\n",
              " [42034, 42669, 20154, 789],\n",
              " [42470, 41663, 33738, 41201],\n",
              " [40330, 20628, 49279, 8281],\n",
              " [23428, 12335, 34480, 48],\n",
              " [21527, 36965, 3121, 675],\n",
              " [33966, 28527, 38461, 6468],\n",
              " [17394, 6186, 49232, 11270],\n",
              " [43130, 7104, 22227, 9946],\n",
              " [7177, 46148, 21697, 49557],\n",
              " [32860, 22900, 47683, 47119],\n",
              " [34372, 19142, 42776, 31098],\n",
              " [19168, 28268, 46628, 11492],\n",
              " [1799, 17221, 9268, 7223],\n",
              " [31051, 14019, 19674, 35791],\n",
              " [97, 128, 25884, 49386],\n",
              " [9943, 18412, 18997, 22282],\n",
              " [16758, 2039, 18386, 8191],\n",
              " [42327, 46831, 19607, 20465],\n",
              " [8441, 30590, 44583, 49672],\n",
              " [45311, 12661, 4411, 15713],\n",
              " [30010, 10215, 21949, 9948],\n",
              " [16504, 45841, 19181, 3728],\n",
              " [31359, 19540, 6940, 22840],\n",
              " [37527, 37036, 2373, 38028],\n",
              " [27397, 22407, 4003, 7249],\n",
              " [9918, 20966, 33696, 19818],\n",
              " [15172, 34127, 24132, 7081],\n",
              " [28199, 10006, 26172, 48156],\n",
              " [1534, 17586, 46322, 20649],\n",
              " [45481, 27100, 45993, 43391],\n",
              " [44557, 46919, 3467, 16267],\n",
              " [45862, 43646, 49613, 7321],\n",
              " [25178, 35922, 16826, 7378],\n",
              " [38232, 46989, 5378, 38493],\n",
              " [35135, 18424, 26790, 41108],\n",
              " [11611, 35796, 37745, 39374],\n",
              " [28785, 4094, 30760, 7095],\n",
              " [10188, 43925, 33413, 26074],\n",
              " [15442, 14979, 44825, 14239],\n",
              " [31454, 1206, 598, 16200],\n",
              " [8232, 14055, 37698, 32850],\n",
              " [14981, 27499, 5945, 11781],\n",
              " [13578, 31550, 2966, 38094],\n",
              " [4682, 45488, 43280, 8735],\n",
              " [48749, 18967, 45031, 35087],\n",
              " [40430, 18090, 49692, 38818],\n",
              " [35149, 653, 16561, 4240],\n",
              " [37682, 15879, 37318, 1354],\n",
              " [886, 20681, 18426, 11335],\n",
              " [24897, 46489, 10281, 31860],\n",
              " [19634, 32549, 37878, 1113],\n",
              " [33943, 10357, 14476, 4202],\n",
              " [9871, 17116, 32271, 42692],\n",
              " [22324, 19617, 8993, 36922],\n",
              " [47389, 42748, 45820, 34194],\n",
              " [14197, 45546, 8371, 16400],\n",
              " [4478, 37215, 9280, 23762],\n",
              " [10289, 21828, 29111, 14148],\n",
              " [7574, 776, 37536, 39576],\n",
              " [24539, 30838, 36144, 11357],\n",
              " [27665, 5981, 31052, 26873],\n",
              " [26293, 46146, 37804, 1986],\n",
              " [7641, 23031, 24989, 19927],\n",
              " [46916, 7090, 12797, 46749],\n",
              " [14177, 9778, 3371, 13439],\n",
              " [11992, 28405, 29221, 14810],\n",
              " [29354, 17739, 27584, 23049],\n",
              " [5317, 25641, 16464, 30604],\n",
              " [22565, 37642, 24768, 27898],\n",
              " [46766, 11448, 14914, 1028],\n",
              " [27271, 22070, 44986, 10025],\n",
              " [2387, 43511, 29360, 3617],\n",
              " [32028, 5967, 41119, 37176],\n",
              " [43021, 46240, 13690, 31500],\n",
              " [25540, 22751, 48942, 7495],\n",
              " [43219, 25147, 19773, 40396],\n",
              " [21085, 13257, 46801, 38881],\n",
              " [40884, 31833, 20249, 9470],\n",
              " [31803, 10246, 31504, 10813],\n",
              " [4781, 23272, 5238, 24673],\n",
              " [40618, 12556, 48243, 10787],\n",
              " [48509, 23779, 8430, 19731],\n",
              " [27068, 24322, 21883, 3689],\n",
              " [17789, 36730, 28482, 44019],\n",
              " [27783, 35157, 19998, 23898],\n",
              " [9317, 6197, 9054, 11334],\n",
              " [35454, 27225, 39369, 29207],\n",
              " [16827, 5904, 36184, 14758],\n",
              " [9784, 14804, 25916, 31682],\n",
              " [7846, 38265, 11232, 45158],\n",
              " [30945, 20594, 39557, 15875],\n",
              " [45326, 17515, 39650, 32658],\n",
              " [34410, 9706, 21926, 31023],\n",
              " [2509, 13813, 3857, 18493],\n",
              " [11240, 31705, 7996, 36593],\n",
              " [5468, 45289, 37977, 279],\n",
              " [47557, 16367, 19058, 28698],\n",
              " [40812, 19970, 35201, 13310],\n",
              " [43799, 14923, 7884, 31345],\n",
              " [27542, 26672, 28195, 18131],\n",
              " [31982, 22024, 34149, 7115],\n",
              " [18735, 34737, 22843, 40089],\n",
              " [34918, 25308, 3166, 47309],\n",
              " [1785, 21947, 182, 29956],\n",
              " [8487, 36498, 38897, 31380],\n",
              " [22336, 3468, 42873, 41990],\n",
              " [31609, 31583, 39347, 36086],\n",
              " [34695, 30704, 21322, 49816],\n",
              " [14261, 47951, 9797, 43813],\n",
              " [40690, 48328, 47349, 34900],\n",
              " [29357, 24164, 5034, 12444],\n",
              " [14520, 16431, 11331, 15196],\n",
              " [19013, 19791, 35147, 3553],\n",
              " [44925, 29723, 18736, 34540],\n",
              " [23517, 25526, 26281, 37296],\n",
              " [1676, 21156, 47175, 48528],\n",
              " [45853, 43572, 28765, 30098],\n",
              " [36636, 16972, 12760, 3561],\n",
              " [33492, 12941, 25008, 9823],\n",
              " [26068, 9230, 6764, 32631],\n",
              " [39398, 23051, 9678, 17327],\n",
              " [33673, 28048, 24437, 27705],\n",
              " [24799, 11790, 35873, 6905],\n",
              " [18409, 33294, 46017, 4422],\n",
              " [25914, 8013, 14074, 5874],\n",
              " [46942, 14221, 44403, 31666],\n",
              " [19756, 935, 30632, 8504],\n",
              " [32349, 28852, 26591, 21763],\n",
              " [33549, 3078, 31829, 34917],\n",
              " [14600, 21211, 9198, 15883],\n",
              " [42205, 48123, 32445, 42969],\n",
              " [45700, 11298, 44311, 2780],\n",
              " [12784, 40181, 31367, 26737],\n",
              " [39240, 14453, 24910, 49230],\n",
              " [5340, 49506, 9262, 37985],\n",
              " [29812, 6497, 17741, 49931],\n",
              " [11731, 14381, 12631, 38452],\n",
              " [26968, 5750, 380, 12498],\n",
              " [27480, 18650, 39635, 37001],\n",
              " [3158, 3118, 42725, 41557],\n",
              " [38467, 19513, 36070, 924],\n",
              " [19038, 45131, 29439, 6156],\n",
              " [35215, 458, 48278, 33565],\n",
              " [1371, 39593, 27533, 40995],\n",
              " [13931, 8594, 40174, 8357],\n",
              " [20231, 19652, 15387, 23808],\n",
              " [48804, 6956, 33827, 48583],\n",
              " [10656, 41078, 14250, 3171],\n",
              " [22690, 19915, 43986, 5022],\n",
              " [15736, 22454, 25617, 24291],\n",
              " [44513, 41279, 28001, 35607],\n",
              " [30331, 46506, 40070, 31090],\n",
              " [36550, 4872, 41499, 23553],\n",
              " [18459, 28229, 46779, 34627],\n",
              " [316, 4026, 1385, 21373],\n",
              " [24085, 23062, 34793, 35645],\n",
              " [15567, 13069, 11086, 42421],\n",
              " [40463, 35696, 13826, 5510],\n",
              " [38603, 1552, 10542, 46731],\n",
              " [7425, 26231, 46468, 24755],\n",
              " [19650, 30517, 12917, 48407],\n",
              " [7594, 43228, 7403, 21013],\n",
              " [42794, 40320, 35957, 15353],\n",
              " [14521, 40651, 17792, 12809],\n",
              " [18198, 38516, 45265, 35169],\n",
              " [40147, 3994, 37180, 29594],\n",
              " [48684, 8285, 46553, 7670],\n",
              " [37137, 49880, 33234, 16056],\n",
              " [23035, 29724, 34463, 136],\n",
              " [44459, 37512, 21244, 14411],\n",
              " [12861, 2016, 4669, 41684],\n",
              " [27623, 36799, 44508, 28231],\n",
              " [36961, 16616, 12915, 44958],\n",
              " [4348, 39729, 26633, 49127],\n",
              " [36535, 20081, 21994, 24135],\n",
              " [30744, 39035, 36287, 17046],\n",
              " [20167, 6655, 7992, 32303],\n",
              " [5008, 31540, 4069, 22064],\n",
              " [11006, 14217, 5875, 25721],\n",
              " [32135, 22666, 31742, 5935],\n",
              " [31242, 13049, 25934, 43933],\n",
              " [37072, 39634, 4574, 29302],\n",
              " [12690, 26750, 26325, 19484],\n",
              " [2351, 22884, 7266, 49309],\n",
              " [42405, 10557, 43482, 29122],\n",
              " [5889, 34254, 49677, 15244],\n",
              " [35094, 10134, 21877, 29587],\n",
              " [17087, 39272, 35590, 7784],\n",
              " [8720, 5666, 11409, 48132],\n",
              " [17952, 25987, 2776, 37641],\n",
              " [33018, 43328, 38299, 48532],\n",
              " [35801, 16835, 44540, 8348],\n",
              " [14971, 25114, 25935, 14332],\n",
              " [45789, 38651, 45064, 7740],\n",
              " [44372, 18741, 45666, 43778],\n",
              " [31164, 1135, 49696, 16010],\n",
              " [20885, 12928, 44727, 567],\n",
              " [48351, 47438, 17979, 12623],\n",
              " [33113, 30795, 49909, 5571],\n",
              " [39141, 8016, 33960, 31376],\n",
              " [8322, 4189, 16215, 11844],\n",
              " [21652, 12612, 35692, 15720],\n",
              " [6815, 43768, 21773, 36392],\n",
              " [16117, 14818, 6862, 25742],\n",
              " [30743, 1324, 32775, 4198],\n",
              " [36632, 17826, 35992, 16329],\n",
              " [49851, 49701, 18691, 46505],\n",
              " [29841, 18324, 29189, 28090],\n",
              " [11001, 12670, 28580, 12159],\n",
              " [14858, 21622, 14223, 26424],\n",
              " [2199, 26363, 35197, 4594],\n",
              " [35618, 9954, 47163, 46094],\n",
              " [18969, 47477, 37623, 43477],\n",
              " [39070, 24223, 44224, 22553],\n",
              " [15926, 12062, 10451, 22477],\n",
              " [4755, 30141, 35643, 6598],\n",
              " [19581, 3935, 10714, 2160],\n",
              " [42083, 42624, 47415, 37481],\n",
              " [15911, 46323, 28135, 27098],\n",
              " [14674, 49534, 24257, 35775],\n",
              " [17279, 32732, 48363, 42434],\n",
              " [13398, 17915, 48563, 42988],\n",
              " [2517, 32205, 43571, 16465],\n",
              " [8514, 11415, 41583, 37597],\n",
              " [9507, 7935, 16688, 9066],\n",
              " [29862, 39025, 15834, 39478],\n",
              " [48600, 31592, 1430, 10223],\n",
              " [35248, 12226, 4306, 19015],\n",
              " [49932, 19706, 34558, 5919],\n",
              " [848, 13321, 2278, 36199],\n",
              " [22761, 23187, 45262, 40140],\n",
              " [49708, 41127, 6131, 40694],\n",
              " [35178, 80, 11128, 29026],\n",
              " [15189, 24494, 31479, 36020],\n",
              " [45401, 31870, 33310, 5127],\n",
              " [19278, 11528, 27689, 44854],\n",
              " [43154, 15884, 21489, 3812],\n",
              " [34828, 40593, 30763, 21332],\n",
              " [35839, 22009, 8069, 5977],\n",
              " [17837, 12387, 45528, 29301],\n",
              " [46893, 34859, 27097, 29500],\n",
              " [23802, 12662, 13118, 14692],\n",
              " [47289, 31579, 43344, 11952],\n",
              " [26112, 2855, 13496, 12257],\n",
              " [19506, 37303, 9935, 48605],\n",
              " [18903, 24482, 24372, 11221],\n",
              " [9681, 13746, 13708, 32811],\n",
              " [45073, 28070, 14715, 3495],\n",
              " [2470, 10984, 39879, 31348],\n",
              " [41022, 1849, 46287, 437],\n",
              " [38033, 10003, 28028, 44389],\n",
              " [13114, 40115, 47047, 29099],\n",
              " [47914, 13190, 44474, 2592],\n",
              " [42247, 7978, 28683, 3890],\n",
              " [47916, 12900, 33833, 44912],\n",
              " [33773, 31285, 13174, 20006],\n",
              " [41505, 33000, 3466, 44445],\n",
              " [21849, 5253, 36773, 37795],\n",
              " [19065, 46556, 38958, 25252],\n",
              " [42824, 22447, 40318, 34588],\n",
              " [21022, 45735, 14437, 12539],\n",
              " [38121, 1718, 43341, 42866],\n",
              " [30048, 48940, 39713, 24075],\n",
              " [12658, 16453, 36259, 24909],\n",
              " [7156, 27308, 40891, 11752],\n",
              " [48076, 30276, 5184, 30934],\n",
              " [36923, 21996, 4170, 11517],\n",
              " [10534, 8078, 7427, 32647],\n",
              " [20044, 47148, 26328, 22714],\n",
              " [21614, 25815, 18836, 5156],\n",
              " [28294, 34100, 35629, 19219],\n",
              " [26162, 98, 37744, 38505],\n",
              " [17905, 9131, 6043, 40848],\n",
              " [29082, 44407, 32006, 2020],\n",
              " [25797, 14738, 30516, 12807],\n",
              " [42803, 19520, 17262, 7268],\n",
              " [475, 13237, 6304, 7897],\n",
              " [24338, 27156, 31524, 35385],\n",
              " [44929, 37630, 45798, 5074],\n",
              " [23781, 48010, 20473, 16877],\n",
              " [39247, 30457, 14275, 23135],\n",
              " [41718, 24590, 16911, 45433],\n",
              " [29091, 46055, 34156, 23856],\n",
              " [40502, 23393, 5409, 31488],\n",
              " [24224, 6900, 5719, 27509],\n",
              " [12312, 5778, 2116, 35832],\n",
              " [32975, 27803, 22807, 11218],\n",
              " [17677, 36817, 49824, 30506],\n",
              " [39428, 43221, 46950, 46172],\n",
              " [18917, 11732, 45978, 19678],\n",
              " [25651, 16466, 4591, 40309],\n",
              " [15784, 21981, 26323, 17736],\n",
              " [19971, 12238, 5126, 42088],\n",
              " [38870, 45638, 33114, 46116],\n",
              " [6435, 2136, 33033, 44532],\n",
              " [3072, 8970, 26954, 3980],\n",
              " [42003, 29879, 35623, 30140],\n",
              " [32383, 20897, 17757, 13770],\n",
              " [28692, 12890, 8112, 9705],\n",
              " [44576, 8524, 29185, 18480],\n",
              " [13140, 10462, 40286, 10568],\n",
              " [19313, 28655, 47778, 9223],\n",
              " [27479, 30680, 19235, 39528],\n",
              " [1188, 40252, 4573, 48379],\n",
              " [20788, 25893, 25532, 46498],\n",
              " [19953, 20997, 5456, 33783],\n",
              " [38238, 38644, 27725, 11549],\n",
              " [48209, 8816, 16064, 3819],\n",
              " [3280, 43456, 40727, 41790],\n",
              " [2588, 44754, 11374, 29120],\n",
              " [8848, 9415, 48339, 22268],\n",
              " [7793, 39403, 7711, 29173],\n",
              " [22700, 38036, 2627, 5280],\n",
              " [26922, 4139, 23723, 17611],\n",
              " [7776, 5155, 15647, 7848],\n",
              " [45008, 22793, 7035, 45942],\n",
              " [46702, 13859, 5032, 39989],\n",
              " [23201, 5503, 4342, 21825],\n",
              " [11476, 4313, 14271, 894],\n",
              " [43846, 41008, 41917, 45688],\n",
              " [13979, 33566, 28282, 37764],\n",
              " [3652, 21423, 20415, 19100],\n",
              " [21122, 21964, 46134, 15676],\n",
              " [8092, 11193, 8834, 23098],\n",
              " [15216, 42496, 44197, 31614],\n",
              " [11621, 22153, 39745, 22426],\n",
              " [8297, 22249, 45649, 35500],\n",
              " [37248, 28021, 37081, 45800],\n",
              " [45868, 20216, 45918, 19939],\n",
              " [36034, 4242, 34523, 40719],\n",
              " [31990, 9443, 47370, 38700],\n",
              " [33419, 35063, 33519, 25727],\n",
              " [47410, 32138, 11723, 5989],\n",
              " [47032, 31965, 13150, 34412],\n",
              " [40414, 21948, 4625, 29903],\n",
              " [29493, 16441, 43619, 12056],\n",
              " [4133, 44700, 23138, 39758],\n",
              " [27881, 7374, 9856, 41247],\n",
              " [22944, 5427, 30555, 48256],\n",
              " [4967, 6775, 22681, 2696],\n",
              " [33036, 37148, 44419, 13423],\n",
              " [19881, 33529, 23843, 10645],\n",
              " [9834, 32407, 33555, 15763],\n",
              " [2888, 46547, 26511, 40455],\n",
              " [16162, 9060, 19560, 29779],\n",
              " [27510, 34718, 33055, 43171],\n",
              " [8015, 3070, 5117, 45013],\n",
              " [6744, 12006, 45233, 4263],\n",
              " [7686, 31596, 15955, 34458],\n",
              " [12794, 446, 16666, 41692],\n",
              " [20837, 18442, 26858, 30068],\n",
              " [23333, 921, 15569, 12688],\n",
              " [26309, 32700, 48050, 39485],\n",
              " [40150, 5816, 34096, 5161],\n",
              " [11817, 12853, 41467, 9825],\n",
              " [22362, 14911, 6947, 28610],\n",
              " [49541, 49316, 42120, 32223],\n",
              " [30812, 47737, 28418, 45301],\n",
              " [1357, 45587, 16289, 39587],\n",
              " [36815, 11237, 21713, 47612],\n",
              " [24791, 26516, 37388, 36385],\n",
              " [42397, 15107, 1637, 34654],\n",
              " [44343, 38234, 4175, 49134],\n",
              " [25850, 20839, 27906, 1131],\n",
              " [37996, 17002, 45647, 25137],\n",
              " [12824, 40287, 5416, 22181],\n",
              " [19578, 20440, 18227, 14172],\n",
              " [45808, 3348, 44928, 19084],\n",
              " [43847, 21806, 3878, 33617],\n",
              " [45600, 25931, 40039, 31425],\n",
              " [20046, 5274, 27319, 30663],\n",
              " [12828, 27477, 35053, 26107],\n",
              " [33411, 21924, 30115, 15146],\n",
              " [12672, 32737, 48858, 37328],\n",
              " [4035, 15824, 32698, 34844],\n",
              " [24136, 39100, 39755, 47818],\n",
              " [25321, 1883, 35207, 4620],\n",
              " [8950, 48731, 22964, 5296],\n",
              " [28494, 35763, 37570, 38941],\n",
              " [18533, 15000, 27521, 49257],\n",
              " [27486, 47230, 16880, 9321],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "yuDZ5V_Y8OeK"
      },
      "outputs": [],
      "source": [
        "def collate(b):\n",
        "    xs,ys = zip(*b)\n",
        "    return torch.stack(xs),torch.stack(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "7KSfGJ3c8OeK"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
        "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "RDTQMUA78OeK"
      },
      "outputs": [],
      "source": [
        "train_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\n",
        "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "1Od0RTFV8OeK"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
        "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "B3maM8VY8OeK",
        "outputId": "39dc3bc7-fb49-4c62-8f8e-325918e0f740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(valid_dl))"
      ],
      "metadata": {
        "id": "JgSjCeF9Ati-",
        "outputId": "2310bcdb-0e08-449b-b017-9ddf0cda507f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n",
              "         8, 3, 7, 7, 8, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psJJ0oxN8OeL",
        "outputId": "f18ad2d7-e9b6-4508-c5df-34798fd61f79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([50, 784]), torch.Size([50]))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xb.shape,yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS8ODuDU8OeL"
      },
      "outputs": [],
      "source": [
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tjNdf6o8OeL",
        "outputId": "d1fe34ca-dc0b-46b0-cb35-26e0a3e36ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.27, 0.04\n",
            "0.15, 0.02\n",
            "0.01, 0.12\n"
          ]
        }
      ],
      "source": [
        "fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sYQOClk8OeL"
      },
      "source": [
        "### Multiprocessing DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "073fpCla8OeL"
      },
      "outputs": [],
      "source": [
        "import torch.multiprocessing as mp\n",
        "from fastcore.basics import store_attr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "nfeWy_7c8OeL",
        "outputId": "7a1212cc-4b7e-42e2-ffb1-fa39aa4b3e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([1, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ],
      "source": [
        "train_ds[[3,6,8,1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "jBc979Ct8OeL",
        "outputId": "782150c0-b2a4-4af6-f417-cef33cfdaefb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([1, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ],
      "source": [
        "train_ds.__getitem__([3,6,8,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "vFNkxvCI8OeM",
        "outputId": "2f5e9670-0fa9-46c3-b962-0b89acd52fd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
          ]
        }
      ],
      "source": [
        "for o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "gv6-TPrl8OeM"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n",
        "    def __iter__(self):\n",
        "        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "T_I3Nk_Z8OeM"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\n",
        "it = iter(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "nOdMQVsf8OeM",
        "outputId": "73b426ed-25ab-44b9-e481-1eecba03335e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50, 784]),\n",
              " torch.Size([50]),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([6, 3, 6, 1, 1, 7, 0, 7, 4, 4, 2, 5, 4, 8, 4, 9, 1, 5, 6, 4, 7, 1, 8, 2, 8, 4, 1, 6, 1, 3, 5, 8, 6, 6, 0, 0, 0, 3, 3, 6, 5, 4, 0, 8,\n",
              "         2, 1, 7, 9, 3, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "xb,yb = next(it)\n",
        "xb.shape,yb.shape, xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(it)"
      ],
      "metadata": {
        "id": "UUL80zTUWtUh",
        "outputId": "3a6fbb3e-1a47-486c-f7b3-84e2bf3ddade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([4, 1, 9, 8, 7, 9, 5, 2, 6, 2, 2, 1, 8, 8, 8, 3, 0, 8, 8, 0, 3, 1, 4, 6, 1, 2, 6, 7, 9, 2, 2, 8, 9, 8, 5, 9, 0, 9, 4, 7, 1, 0, 3, 9,\n",
              "         9, 9, 6, 2, 7, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "td = (tensor([[0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0., 0.]]),\n",
        " tensor([4, 1, 9, 8, 7, 9]))"
      ],
      "metadata": {
        "id": "N4cv5oA6XJ-h"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate()"
      ],
      "metadata": {
        "id": "BD9_CM44XpYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvdnbk0u8OeM"
      },
      "source": [
        "### PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "FYI2yfCq8OeN"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "L8Dgxr3y8OeN"
      },
      "outputs": [],
      "source": [
        "train_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\n",
        "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "Kj_wBYcK8OeN"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(train_dl)\n",
        "\n",
        "xb,yb = next(it)\n",
        "xb.shape,yb.shape, xb, yb"
      ],
      "metadata": {
        "id": "uORYLM6OYD1a",
        "outputId": "3862eab9-1fef-43ed-fd13-95148bb1dd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50, 784]),\n",
              " torch.Size([50]),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([8, 1, 4, 3, 0, 9, 3, 1, 3, 8, 9, 1, 3, 3, 9, 1, 1, 7, 9, 0, 6, 3, 1, 1, 3, 7, 3, 8, 4, 7, 9, 9, 1, 9, 1, 3, 3, 3, 4, 0, 8, 7, 1, 9,\n",
              "         6, 5, 9, 1, 1, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "0hSn9wWx8OeN",
        "outputId": "b8508f0b-1496-4211-f22b-b6823308f840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12, 0.96\n",
            "0.11, 0.98\n",
            "0.06, 0.98\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.15, grad_fn=<NllLossBackward0>), tensor(0.94))"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcfgWvte8OeN"
      },
      "source": [
        "PyTorch can auto-generate the BatchSampler for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "7lyzcuhU8OeN"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3lkejP8OeO"
      },
      "source": [
        "PyTorch can also generate the Sequential/RandomSamplers too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "DRMKYyCm8OeO"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
        "valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "29i52TZy8OeO",
        "outputId": "9b1414c8-5da6-4730-f2d9-c3606004c063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28, 0.88\n",
            "0.17, 0.94\n",
            "0.20, 0.94\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.40, grad_fn=<NllLossBackward0>), tensor(0.90))"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW-7JCVI8OeO"
      },
      "source": [
        "Our dataset actually already knows how to sample a batch of indices all at once:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds??"
      ],
      "metadata": {
        "id": "SrSsRayBaZI4"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "rdy9BTXU8OeO",
        "outputId": "4be0799d-ea40-420b-a0af-aca0c3e6ba53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([9, 1, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ],
      "source": [
        "train_ds[[4,6,7]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkYBASPQ8OeO"
      },
      "source": [
        "...that means that we can actually skip the batch_sampler and collate_fn entirely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "dIkM6EWF8OeO"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, sampler=train_samp)\n",
        "valid_dl = DataLoader(valid_ds, sampler=valid_samp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "5k6eM5Ke8OeP",
        "outputId": "3d03f5f1-b269-4812-a50c-9819f61d6a43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ],
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "xb.shape,yb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBnbS9N8OeP"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEHdlrJF8OeP"
      },
      "source": [
        "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
        "\n",
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit??"
      ],
      "metadata": {
        "id": "u52vIUP6fmQM"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "OUMRW5tU8OeP"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb,yb in train_dl:\n",
        "            loss = loss_func(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc,count = 0.,0.,0\n",
        "            for xb,yb in valid_dl:\n",
        "                pred = model(xb)\n",
        "                n = len(xb)\n",
        "                count += n\n",
        "                tot_loss += loss_func(pred,yb).item()*n\n",
        "                tot_acc  += accuracy (pred,yb).item()*n\n",
        "        print(epoch, tot_loss/count, tot_acc/count)\n",
        "    return tot_loss/count, tot_acc/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "JDTodKFh8OeP"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyWe0WbM8OeQ"
      },
      "source": [
        "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "Aee3pMu38OeQ"
      },
      "outputs": [],
      "source": [
        "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
        "model,opt = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "BuzeGQx48OeQ",
        "outputId": "eca958ec-0006-4bd3-904b-3798743e9546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.23751522440463305 0.9227000027894974\n",
            "1 0.11702211963012815 0.9653000050783157\n",
            "2 0.11938467755448073 0.9637000077962875\n",
            "3 0.11142657274380326 0.9677000045776367\n",
            "4 0.11633713906281627 0.967400004863739\n",
            "CPU times: user 8.02 s, sys: 0 ns, total: 8.02 s\n",
            "Wall time: 8.14 s\n"
          ]
        }
      ],
      "source": [
        "%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9m4WBu8OeQ"
      },
      "source": [
        "## Export -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR1Jf8jA8OeQ"
      },
      "outputs": [],
      "source": [
        "import nbdev; nbdev.nbdev_export()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smUz6Y6w8OeQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}